{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文自然语言处理分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和拉丁语系不同，亚洲语言是不用空格分开每个有意义的词的。而当我们进行自然语言处理的时候，大部分情况下，词汇是我们对句子和文章理解的基础，因此需要一个工具去把完整的文本中分解成粒度更细的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于 TF-IDF 算法的关键词抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jieba.analyse\n",
    "\n",
    "* jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())  \n",
    "  * sentence 为待提取的文本  \n",
    "  * topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20  \n",
    "  * withWeight 为是否一并返回关键词权重值，默认值为 False  \n",
    "  * allowPOS 仅包括指定词性的词，默认值为空，即不筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/1q/57zhs1tx0230_c9b_0hc055m0000gn/T/jieba.cache\n",
      "Loading model cost 0.795 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户  2016  互联网  手机  平台  人工智能  百度  2017  智能  技术  数据  360  服务  直播  产品  企业  安全  视频  移动  应用  网络  行业  游戏  机器人  电商  内容  中国  领域  通过  发展\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/technology_news.csv\", encoding='utf-8')  # 读取科技新闻\n",
    "df = df.dropna()  # 老规矩，去掉缺省数据\n",
    "lines=df.content.values.tolist()  # 转化为list形式\n",
    "content = \"\".join(lines)  # 将序列中的元素以指定的字符连接生成一个新的字符串\n",
    "print(\"  \".join(analyse.extract_tags(content, topK=30, withWeight=False, allowPOS=())))  # 抽取出30个权重最高的词进行打印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "航母  训练  海军  中国  官兵  部队  编队  作战  10  任务  美国  导弹  能力  20  2016  军事  无人机  装备  进行  记者  我们  军队  安全  保障  12  战略  军人  日本  南海  战机\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8')\n",
    "df = df.dropna()\n",
    "lines=df.content.values.tolist()\n",
    "content = \"\".join(lines)\n",
    "print(\"  \".join(analyse.extract_tags(content, topK=30, withWeight=False, allowPOS=())))  # analyse.extract_tags的运用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于 TextRank 算法的关键词抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。\n",
    "* jieba.analyse.TextRank() 新建自定义 TextRank 实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法论文：[TextRank: Bringing Order into Texts](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf \"TextRank: Bringing Order into Texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本思想:\n",
    "\n",
    "* 将待抽取关键词的文本进行分词\n",
    "* 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图\n",
    "* 计算图中节点的PageRank，注意是无向带权图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国  海军  训练  美国  部队  进行  官兵  航母  作战  任务  能力  军事  发展  工作  国家  问题  建设  导弹  编队  记者\n",
      "---------------------我是分割线----------------\n",
      "中国  海军  美国  部队  官兵  航母  军事  国家  任务  能力  导弹  技术  问题  日本  军队  编队  装备  系统  记者  战略\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8')\n",
    "df = df.dropna()\n",
    "lines=df.content.values.tolist()\n",
    "content = \"\".join(lines)\n",
    "\n",
    "print(\"  \".join(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))))\n",
    "print(\"---------------------我是分割线----------------\")  # analyse.textrank（其余与之前一样）\n",
    "print(\"  \".join(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.LDA主题模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "咱们来用LDA主题模型建模，看看这些新闻主要在说哪些topic。\n",
    "\n",
    "首先我们要把文本内容处理成固定的格式，一个包含句子的list，list中每个元素是分词后的词list。类似下面这个样子。\n",
    "\n",
    "[[第，一，条，新闻，在，这里],[第，二，条，新闻，在，这里],[这，是，在，做， 什么],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords=pd.read_csv(\"data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "stopwords=stopwords['stopword'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换成合适的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/technology_news.csv\", encoding='utf-8')\n",
    "df = df.dropna()\n",
    "lines=df.content.values.tolist()\n",
    "\n",
    "sentences=[]  # 存储了分词之后的词组\n",
    "for line in lines:\n",
    "    try:\n",
    "        segs=jieba.lcut(line)\n",
    "        # 过滤掉停用词、空值以及换行符\n",
    "        segs = filter(lambda x:len(x)>1, segs)  \n",
    "        # 注意此时如果不将其强制转化为list的话其格式为filter，不能进行下一步操作\n",
    "        segs = list(filter(lambda x:x not in stopwords, segs))  \n",
    "        sentences.append(segs)\n",
    "    except Exception as e:\n",
    "        print(line)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本次\n",
      "商汤\n",
      "带来\n",
      "黄仁勋\n",
      "展示\n",
      "遥相呼应\n",
      "SenseFace\n",
      "人脸\n",
      "布控\n",
      "系统\n",
      "千万级\n",
      "人员\n",
      "库中\n",
      "300ms\n",
      "识别\n",
      "瞬间\n",
      "锁定目标\n",
      "功耗\n",
      "十几\n",
      "当属\n",
      "人脸\n",
      "布控\n",
      "一大\n",
      "科技\n"
     ]
    }
   ],
   "source": [
    "# print(type(sentences))\n",
    "for word in sentences[5]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(sentences)  # 创建词袋模型\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in sentences]  # 利用doc2bow对其进行分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21, 1),\n",
       " (39, 1),\n",
       " (61, 1),\n",
       " (68, 1),\n",
       " (78, 1),\n",
       " (82, 1),\n",
       " (91, 1),\n",
       " (92, 1),\n",
       " (103, 1),\n",
       " (104, 2),\n",
       " (105, 2),\n",
       " (124, 1),\n",
       " (129, 1),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 1),\n",
       " (137, 1),\n",
       " (138, 1)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们查一下第3号分类，其中最常出现的单词是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把所有的主题打印出来看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030*\"数据\" + 0.029*\"用户\" + 0.018*\"360\" + 0.015*\"攻击\" + 0.014*\"系统\" + 0.013*\"漏洞\" + 0.011*\"服务\" + 0.010*\"信息\"\n",
      "0.026*\"城市\" + 0.012*\"北京\" + 0.009*\"社交\" + 0.009*\"老师\" + 0.008*\"上海\" + 0.008*\"医疗\" + 0.008*\"母婴\" + 0.006*\"医生\"\n",
      "0.025*\"手机\" + 0.014*\"支付\" + 0.013*\"用户\" + 0.010*\"电脑\" + 0.007*\"短信\" + 0.006*\"电话\" + 0.006*\"隐私\" + 0.006*\"信息\"\n",
      "0.012*\"流量\" + 0.009*\"媒体\" + 0.008*\"大学生\" + 0.007*\"大学\" + 0.007*\"学院\" + 0.007*\"科研\" + 0.007*\"阅读\" + 0.006*\"文娱\"\n",
      "0.024*\"公司\" + 0.022*\"亿元\" + 0.020*\"网络\" + 0.013*\"诈骗\" + 0.013*\"小米\" + 0.009*\"乐视\" + 0.008*\"万元\" + 0.008*\"电信\"\n",
      "0.011*\"黑客\" + 0.009*\"文件\" + 0.009*\"利用\" + 0.008*\"网络\" + 0.007*\"技术\" + 0.007*\"一带\" + 0.007*\"套餐\" + 0.006*\"一路\"\n",
      "0.021*\"病毒\" + 0.014*\"驾驶\" + 0.014*\"汽车\" + 0.013*\"自动\" + 0.012*\"设计\" + 0.012*\"技术\" + 0.011*\"识别\" + 0.009*\"智能\"\n",
      "0.028*\"服务\" + 0.026*\"智能\" + 0.013*\"平台\" + 0.013*\"工作\" + 0.012*\"企业\" + 0.012*\"生活\" + 0.011*\"互联网\" + 0.010*\"办公\"\n",
      "0.009*\"信息\" + 0.009*\"防御\" + 0.008*\"委员会\" + 0.007*\"手机\" + 0.007*\"数据\" + 0.007*\"输入法\" + 0.006*\"样本\" + 0.006*\"用户\"\n",
      "0.062*\"游戏\" + 0.034*\"用户\" + 0.024*\"手机\" + 0.012*\"报告\" + 0.011*\"玩家\" + 0.011*\"软件\" + 0.007*\"独立\" + 0.006*\"发布\"\n",
      "0.024*\"科技\" + 0.016*\"永恒\" + 0.011*\"研发\" + 0.008*\"产品\" + 0.007*\"妈妈\" + 0.006*\"蓝色\" + 0.006*\"电视\" + 0.006*\"联网\"\n",
      "0.041*\"市场\" + 0.022*\"手机\" + 0.020*\"品牌\" + 0.018*\"中国\" + 0.018*\"产品\" + 0.015*\"联想\" + 0.014*\"增长\" + 0.012*\"业务\"\n",
      "0.011*\"新闻\" + 0.010*\"互联网\" + 0.010*\"猎豹\" + 0.009*\"数据中心\" + 0.009*\"青年\" + 0.007*\"内容\" + 0.006*\"媒体\" + 0.006*\"用户\"\n",
      "0.029*\"智能\" + 0.009*\"智慧\" + 0.008*\"电视\" + 0.007*\"处理器\" + 0.007*\"芯片\" + 0.007*\"机器人\" + 0.007*\"系列\" + 0.007*\"Windows\"\n",
      "0.019*\"游戏\" + 0.015*\"宽带\" + 0.013*\"5G\" + 0.012*\"IP\" + 0.010*\"网络\" + 0.008*\"棋牌\" + 0.008*\"动漫\" + 0.007*\"知识产权\"\n",
      "0.032*\"人工智能\" + 0.031*\"技术\" + 0.029*\"百度\" + 0.013*\"领域\" + 0.012*\"发展\" + 0.011*\"未来\" + 0.009*\"互联网\" + 0.008*\"科技\"\n",
      "0.021*\"手机\" + 0.011*\"用户\" + 0.010*\"信息安全\" + 0.010*\"无人机\" + 0.010*\"安装\" + 0.010*\"共享\" + 0.009*\"单车\" + 0.008*\"系统\"\n",
      "0.048*\"视频\" + 0.045*\"直播\" + 0.019*\"内容\" + 0.018*\"VR\" + 0.016*\"平台\" + 0.013*\"孩子\" + 0.011*\"家长\" + 0.010*\"电竞\"\n",
      "0.022*\"企业\" + 0.021*\"数据\" + 0.019*\"中国\" + 0.017*\"发展\" + 0.013*\"产业\" + 0.013*\"互联网\" + 0.013*\"创新\" + 0.011*\"行业\"\n",
      "0.029*\"腾讯\" + 0.024*\"网络安全\" + 0.017*\"用户\" + 0.014*\"内容\" + 0.014*\"平台\" + 0.010*\"京东\" + 0.007*\"运营\" + 0.006*\"优酷\"\n"
     ]
    }
   ],
   "source": [
    "for topic in lda.print_topics(num_topics=20, num_words=8):\n",
    "    print(topic[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以对新加入的文本，进行简单主题分类：\n",
    "\n",
    "lda.get_document_topics(bow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
